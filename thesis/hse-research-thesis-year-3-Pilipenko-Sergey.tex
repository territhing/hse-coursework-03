\documentclass[a4paper,14pt]{extarticle}
\usepackage{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{tikz}
\usepackage{pgf}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{indentfirst}
\usepackage[colorlinks,citecolor=blue,linkcolor=blue,bookmarks=false,hypertexnames=true, urlcolor=blue]{hyperref} 
\usepackage{indentfirst}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage[flushleft]{threeparttable}
\usepackage{tablefootnote}
\usepackage{csquotes}
\usepackage[backend=bibtex]{biblatex}
\addbibresource{citations}

\usepackage{chngcntr} % нумерация графиков и таблиц по секциям
\counterwithin{table}{section}
\counterwithin{figure}{section}

\graphicspath{{graphics/}}%путь к рисункам

% \makeatletter
% \renewcommand{\@biblabel}[1]{#1.} % Заменяем библиографию с квадратных скобок на точку:
% \makeatother

\geometry{left=2.5cm}% левое поле
\geometry{right=1.5cm}% правое поле
\geometry{top=1.5cm}% верхнее поле
\geometry{bottom=1.5cm}% нижнее поле
\renewcommand{\baselinestretch}{1.5} % междустрочный интервал


% \newcommand{\bibref}[3]{\hyperlink{#1}{#2 (#3)}} % biblabel, authors, year
% \addto\captionsrussian{\def\refname{Список литературы (или источников)}}

\renewcommand{\theenumi}{\arabic{enumi}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\labelenumi}{\arabic{enumi}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\theenumii}{.\arabic{enumii}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\labelenumii}{\arabic{enumi}.\arabic{enumii}.}% Меняем везде перечисления на цифра.цифра
\renewcommand{\theenumiii}{.\arabic{enumiii}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\labelenumiii}{\arabic{enumi}.\arabic{enumii}.\arabic{enumiii}.}% Меняем везде перечисления на цифра.цифра

\begin{document}

\input{thesis-titlepage.tex} % это титульный лист

\newpage

{
	\hypersetup{linkcolor=black}
	\tableofcontents
}

\newpage

\section*{Аннотация}

Восстановление знаков пунктуации в тексте --- актуальная и сложная задача, решение которой необходимо для человеческой обработки текстов, полученных из систем автоматического распознавания речи.
Разметка такого текста может быть неоднозначной, привязанной к контексту, а также очень сильно зависит от сложности самого языка.
Обычно для этой задачи используют двунаправленные рекуррентные нейронные сети.
В последние пару лет эту задачу также пытаются решать с помощью моделей, построенных на архитектуре \enquote{трансформер}.
Цель данной работы --- создание алгоритма, способного восстановить большое количество знаков препинания в русском тексте с возможностью деления текста на абзацы.

\section{Введение}

\subsection{Описание предметной области}

В современных системах распознавания речи входной аудиопоток преобразуется в последовательность слов, разделенных пробелом, для дальнейшей обработки.
Сам по себе такой текст практически не пригоден для чтения обычным человеком, поэтому возникает необходимость в его разметке с помощью знаков пунктуации.
В меру наших знаний, первые попытки решения этой проблемы были предложены Beeferman et al. (1998) \cite{beeferman_cyberpunc:_1998}.
В их статье был предложен алгоритм на основе N-грамм и Марковских цепей для восстановления запятых в тексте, полагаясь только на лексическую информацию из него.

В области же глубинного обучения, такая задача обычно решается с помощью двунаправленных рекуррентных сетей.
Самые популярные архитектуры --- LSTM и GRU.
В работе Tilk и Alumäe (2016) \cite{tilk_bidirectional_2016} рассматривается применение RNN с механизмом внимания для задачи восстановления пунктуации в тексте.
Поскольку авторы этой работы также предоставили код, мы будем использовать их модель как пример рекуррентной сети для сравнительного анализа с итоговым алгоритмом.

Наконец, в последнее время в области обработки естественного языка набрали популярность модели, основанные на архитектуре \enquote{трансформер} \cite{vaswani_attention_2017}.
Наиболее крупное сравнительное исследование для таких моделей, которое нам удалось найти было проведено Tanvirul et al. (2020) \cite{alam_punctuation_2020}, поэтому результаты итоговой модели на корпусе английских текстов будем сравнивать с их результатами.

\subsection{Постановка задачи}

Перед нами была поставлена задача разработки нейронной сети для классификации пробельных символов в последовательности слов без дополнительной разметки. Это предлагается делать опираясь на результаты статьи Michał Pogoda et al. (2021) \cite{pogoda_comprehensive_2021} с полным восстановлением пунктуации.
Словом в рамках нашей задачи считается любая буквенно-цифровая последовательность символов.
Также, в рамках нашей задачи нет ограничения на классы восстанавливаемых знаков препинания.
Таким образом, разметка на знаки препинания будет генерироваться по корпусу тектов, на которых обучается модель, без предварительной разметки и чистки.

Для разработанной модели необходимо будет посчитать качество на англо-русском параллельном корпусе и на наборе данных \enquote{Lenta.ru v1.1+}, чтобы сравнивать результат работы с уже имеющимися моделями.

Затем предстоит оптимизировать скорость предсказаний нейронной сети на CPU, и, возможно, адаптировать архитектуру для того, чтобы моделью можно было пользоваться без GPU.
В качестве базовых моделей рассматриваются silero-models, Bert-Russian-punctuation и neuro-comma.

\subsection{Метрики качества}

Обычные метрики качества в задачах классификации, такие как точность, полнота, $f_{1}$-мера, являются весьма строгими.
Например, если модель предсказывает токен \enquote{.} вместо истинного \enquote{].}, то мы засчитываем это как неудачную классификацию, хотя это не совсем правда.
Чтобы как-то с этим бороться, мы рассмотрим метрику mean Ruzicka similarity (mRS) предложенную Deza и Deza (2009) \cite{deza_encyclopedia_2009}.
Она позволяет нам работать с мультимножествами последовательностей пунктуационных символов (например, \enquote{...}), принимает значения на отрезке $[0, 1]$, и просто считается
\[
	\mathrm{RS}(\mathbf{P}, \mathbf{T}) = \frac{\sum\limits_{k = 1}^{c} \min\left(p_{k}, t_{k}\right)}{\sum\limits_{k = 1}^{c} \max\left(p_{k}, t_{k}\right)},
\]
где
\begin{gather*}
	\mathbf{P} = \left[p_{1}, \ldots, p_{c}\right] \\
	\mathbf{T} = \left[t_{1}, \ldots, t_{c}\right]
\end{gather*}
соответственно предсказанные и истинные метки для одного токена, представленные в форме вектора, состоящего из количества всех единичных символов пунктуации в этом токене, за исключением пробела.

Для вычисления среднего RS мы пользуемся взвешенной RS метрикой по всем меткам, пропуская токены без пунктуации
\[
	\mathrm{mRS} = \frac{\sum\limits_{i = 1}^{N} \mathrm{RS}(\mathbf{P}_{i}, \mathbf{T}_{i})[\mathbf{T}_{i} \neq 0]}{\sum\limits_{i = 1}^{N} [\mathbf{T}_{i} \neq 0]}.
\]

\subsection{Структура работы}

Необходимо перевести корпус текстов \enquote{Lenta.ru v1.1+} в машиночитаемый формат, разметить классы знаков пунктуации, которые нужно будет восстанавливать с помощью алгоритма.
Затем необходимо запустить и обучить базовые модели (Bert-Russian-punctuation, neuro-comma) на выделенном сервере, посчитать качество классификации для разных символов в терминах полноты, точности и $f_{1}$-меры.
Также необходимо будет посчитать метрику mRS для всех базовых моделей.
Потом нужно провести экспериментами с разными архитектурами нейросетей в поисках оптимальной по времени длительности вывода и точности.

\section{Обзор литературы}

Проблема восстановления пунктуации по тексту без разметки стала актуальной в конце 1990-ых годов с появлением первых систем распознавания речи.
Традиционно она решалась с применением N-грамм и цепей Маркова, что позволяло добиться неплохого качества для очень ограниченного набора знаков пунктуации.
Появление рекуррентных нейронных сетей с долговременной и краткосрочной памятью позволило повысить качество предсказаний, и расширить набор знаков пунктуации, которые алгоритмы могут восстановить.
Помимо запятых и точек, стали рассматриваться скобки, восклицательные знаки, вопросительные знаки.

\subsection{LSTM и GRU}

Сети с долговременной и краткосрочной памятью (LSTM) и управляемый рекуррентный блок (GRU) являются популярными архитектурами в задаче восстановления пунктуации.
Не смотря на то, что LSTM превосходят GRU в большинстве задач обработки естественного языка (Yang et al. (2020) \cite{yang_lstm_2020}), GRU лучше себя показывают в контексте работы с пунктуацией (Hládek et al. (2019) \cite{hladek_comparison_2019}).
Помимо того, в силу естественных огранчиений моделей-трансформеров на длину входной последовательности символов, рекуррентные нейронные сети являются более предпочтительной архитектурой в задаче деления текста на абзацы.

\subsection{BERT}

В последние несколько лет лучшие результаты в задачах обработки естественного языка показывают предобученные модели-трансформеры \cite{vaswani_attention_2017}, и задача восстановления пунктуации не исключение.
В меру наших знаний, самый крупный сравнительный анализ таких моделей проводится в статье Alam et al. (2020) \cite{alam_punctuation_2020}, где авторы сравнивают различные варианты предобученных моделей семейства BERT (Devlin et al., 2019 \cite{devlin_no_2019}), RoBERTa (Liu et al., 2019 \cite{devlin_no_2019}) и AlBERT (Lan et al., 2020 \cite{chi_audio_2021}).
Их исследование показывает, что большие модели показывают себя лучше малых, а среди моделей одинакового размера RoBERTa демонстрирует себя лучше чем AlBERT и BERT.

\section{Описание предлагаемого метода}

Целью нашей работы является построение такой модели восстановления пунктуации в тексте, что она способна делать быстрые предсказания на CPU, и не сильно терять в качестве по сравнению с уже существующими моделями для русского языка.
Для разделения текста на параграфы предлагается обучить рекуррентную нейронную сеть, поскольку модели-трансформеры не очень хорошо масштабируются на длинные последовательности символов.
Затем мы дообучим DistilRoBERTa на корпусе текстов \enquote{Lenta.ru v1.1+} для восстановления знаков пунктуации.

\printbibliography
	
\end{document}
